# This workflow will install Python dependencies, run tests and lint with a single version of Python
# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions

name: Python application

on:
  push:
    branches: [ master, v*-branch ]
  pull_request:
    branches: [ master, v*-branch ]

jobs:
  build:

    runs-on: ubuntu-latest

    services:
      mysql:
        image: mysql:5.7
        env:
          MYSQL_ALLOW_EMPTY_PASSWORD: yes
          MYSQL_DATABASE: platiagro
        ports:
        - 3306:3306
        options: --health-cmd="mysqladmin ping" --health-interval=10s --health-timeout=5s --health-retries=3

    steps:
    - uses: actions/checkout@v2

    - name: Set up Python 3.7
      uses: actions/setup-python@v1
      with:
        python-version: 3.7

    - name: Code review tips
      uses: unsplash/comment-on-pr@master
      if: ${{ github.event_name == 'pull_request' && github.event.action == 'opened' }}
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        msg: |
          ## Dicas para revisão de código

          ### Commits
          - Título (1a linha do commit): apresentar resumo do que foi alterado/adicionado/removido.
            ex: adiciona action que salva parametros no backend; exibe rótulo no componente de selecao de dataset;
          - Descrição (outras linhas): dar mais detalhes de cada alteração:
            - motivos das alterações
              ex: havia um bug que causava...; nova funcionalidade que faz isso...; código foi movido para...;
            - bibliotecas adicionadas e versões (requirements.txt)
              ex: atualiza para SQLAlchemy 1.3.20;
            - testes unitários criados/alterados
              ex: adiciona testes para a API PATCH /projects/{projectId}/experiments/{experimentId};
            - alterações do `swagger.yaml`
              ex: adiciona documentação para `GET /projects/{projectId}`
          - Mensagens auto-explicativas! Quem revisa o código deve entender o que foi feito (e porque foi feito) **sem perguntar para quem fez o commit**.
          - Não devem ter conflitos. Solicitar que sejam resolvidas as ocorrências de "This branch has conflicts that must be resolved".

          ### SonarCloud Quality Gate
          - Coverage > 80.0%, e sempre que possível = 100%
          - 0 Bugs, 0 Code Smells, 0 Vulnerabilities
          - São permitidos os seguintes Security Hotspots:
            - Make sure this permissive CORS policy is safe here.
            - Make sure publicly writable directories are used safely here.
            - Using http protocol is insecure. Use https instead.
            - Make sure disabling CSRF protection is safe here.

          ### Build Github actions COM SUCESSO

          ### Python
          - Usar Python>=3.7
          - Remover `print`.
          - Não deixar código-fonte comentado.
          - f-string `f'text-{variable}'` é melhor que `'text-{}'.format(variable)` e `'text-' + variable`
          - Métodos que são chamados de outros arquivos `.py` **DEVEM TER Docstring**.
          - Usar NumPy Style Python Docstring: https://www.sphinx-doc.org/en/master/usage/extensions/example_numpy.html
          - Usar sempre import absoluto.
            ex: from projects.database import Base (BOM), from .database import Base (RUIM)

          ### Padrão de URLs para API REST
          - Usar REST resource naming guide: https://restfulapi.net/resource-naming/
          - USE SUBSTANTIVOS! **NÃO USE VERBOS NA URL!**
            ex: `/projects/:projectId/executions` (BOM), `/project/execute` (RUIM)
          - **SUBSTANTIVOS SEMPRE NO PLURAL!**
            ex: `/deployments/:deploymentId` (BOM), `/deployment/:deploymentId` (RUIM)
          - **SUBSTANTIVOS SÃO SEMPRE SEPARADOS POR UM ID. NÃO USE DOIS SUBSTANTIVOS SEGUIDOS**
            ex: `/experiments/:experimentId/results` (BOM), `/experiments/results/:experimentId` (RUIM)
          - Para consultar uma lista de resources (paginada ou não):
            ex: `GET /resources?page=1&size=10&filter=...`
          - Para criar um resource (e gerar um resourceId aleatório):
            ex: `POST /resources`
          - Para acessar um resource por resourceId:
            ex: `GET /resources/{resourceId}`
          - Para substituir/criar (ou atualizar TODOS OS CAMPOS) de um resource com resourceId específico:
            ex: `PUT /resources/{resourceId}`
          - Para excluir um resource:
            ex: `DELETE /resources/{resourceId}`
          - Para atualizar alguns campos de um resource:
            ex: `PATCH /resources/{resourceId}`
          - Em dúvidas? Mantenha uma consistência com as URLs já existem.

    - name: Install Kubernetes 1.15.7
      run: |
        curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add
        sudo swapoff -a
        sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
        sudo apt-get install -y --allow-downgrades kubeadm=1.15.7-00 kubelet=1.15.7-00 kubectl=1.15.7-00
        echo 'KUBELET_EXTRA_ARGS=--eviction-hard=nodefs.available<1%,nodefs.inodesFree<1%,imagefs.available<1% --image-gc-high-threshold=99' | sudo tee /etc/default/kubelet
        sudo systemctl daemon-reload
        sudo systemctl restart kubelet
        sudo kubeadm init
        sudo sed -i '/^    - --service-account-key-file.*/a \ \ \ \ - --service-account-issuer=kubernetes.default.svc' /etc/kubernetes/manifests/kube-apiserver.yaml
        sudo sed -i '/^    - --service-account-key-file.*/a \ \ \ \ - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key' /etc/kubernetes/manifests/kube-apiserver.yaml
        sudo mkdir -p $HOME/.kube
        sleep 5
        sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
        sudo chown $(id -u):$(id -g) $HOME/.kube/config
        kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
        kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=admin --user=kubelet --group=system:serviceaccounts
        kubectl taint nodes --all node-role.kubernetes.io/master-
        while [ $(kubectl get node | tail -n 1 |awk '{print $2}') != "Ready" ]; do echo waiting nodes to be ready...; sleep 1; done

    - name: Add Persistent Volumes to Kubernetes
      run: |
        for i in `seq 1 10`; do
          sudo mkdir -p "/mnt/disks/vol-$i"
          sudo mount -t tmpfs -o size=20G "vol-$i" "/mnt/disks/vol-$i"
        done
        cat <<EOF | kubectl apply -f -
          kind: StorageClass
          apiVersion: storage.k8s.io/v1
          metadata:
            name: local-storage
          provisioner: kubernetes.io/no-provisioner
          reclaimPolicy: Delete
        EOF
        kubectl patch storageclass local-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
        cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: local-provisioner-config
            namespace: default
            labels:
              heritage: "Tiller"
              release: "release-name"
              chart: provisioner-2.3.2
          data:
            storageClassMap: |
              local-storage:
                hostDir: /mnt/disks
                mountDir: /mnt/disks
                blockCleanerCommand:
                  - "/scripts/shred.sh"
                  - "2"
                volumeMode: Filesystem
        EOF
        cat <<EOF | kubectl apply -f -
          apiVersion: apps/v1
          kind: DaemonSet
          metadata:
            name: local-volume-provisioner
            namespace: default
            labels:
              app: local-volume-provisioner
              heritage: "Tiller"
              release: "release-name"
              chart: provisioner-2.3.2
          spec:
            selector:
              matchLabels:
                app: local-volume-provisioner
            template:
              metadata:
                labels:
                  app: local-volume-provisioner
              spec:
                serviceAccountName: local-storage-admin
                containers:
                  - image: "quay.io/external_storage/local-volume-provisioner:v2.3.2"
                    name: provisioner
                    securityContext:
                      privileged: true
                    env:
                    - name: MY_NODE_NAME
                      valueFrom:
                        fieldRef:
                          fieldPath: spec.nodeName
                    - name: MY_NAMESPACE
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.namespace
                    - name: JOB_CONTAINER_IMAGE
                      value: "quay.io/external_storage/local-volume-provisioner:v2.3.2"
                    volumeMounts:
                      - mountPath: /etc/provisioner/config
                        name: provisioner-config
                        readOnly: true
                      - mountPath: /dev
                        name: provisioner-dev
                      - mountPath: /mnt/disks
                        name: disks
                        mountPropagation: "HostToContainer"
                volumes:
                  - name: provisioner-config
                    configMap:
                      name: local-provisioner-config
                  - name: provisioner-dev
                    hostPath:
                      path: /dev
                  - name: disks
                    hostPath:
                      path: /mnt/disks
        EOF
        cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: local-storage-admin
            namespace: default
            labels:
              heritage: "Tiller"
              release: "release-name"
              chart: provisioner-2.3.2
        EOF
        cat <<EOF | kubectl apply -f -
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: local-storage-provisioner-pv-binding
            labels:
              heritage: "Tiller"
              release: "release-name"
              chart: provisioner-2.3.2
          subjects:
          - kind: ServiceAccount
            name: local-storage-admin
            namespace: default
          roleRef:
            kind: ClusterRole
            name: system:persistent-volume-provisioner
            apiGroup: rbac.authorization.k8s.io
        EOF
        cat <<EOF | kubectl apply -f -
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          metadata:
            name: local-storage-provisioner-node-clusterrole
            labels:
              heritage: "Tiller"
              release: "release-name"
              chart: provisioner-2.3.2
          rules:
          - apiGroups: [""]
            resources: ["nodes"]
            verbs: ["get"]
        EOF
        cat <<EOF | kubectl apply -f -
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: local-storage-provisioner-node-binding
            labels:
              heritage: "Tiller"
              release: "release-name"
              chart: provisioner-2.3.2
          subjects:
          - kind: ServiceAccount
            name: local-storage-admin
            namespace: default
          roleRef:
            kind: ClusterRole
            name: local-storage-provisioner-node-clusterrole
            apiGroup: rbac.authorization.k8s.io
        EOF
        sleep 10
        kubectl wait --for=condition=Ready pods --all --timeout=60s

    - name: Create load balancer
      id: lb
      run: |
        kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.8.3/manifests/metallb.yaml
        KUBEFLOW_MASTER_IP_ADDRESS=$(ifconfig | grep -Eo 'inet (addr:)?([0-9]*\.){3}[0-9]*' | grep -Eo '([0-9]*\.){3}[0-9]*' | grep -v '127.0.0.1' | head -n 1)
        echo ::set-output name=KUBEFLOW_MASTER_IP_ADDRESS::${KUBEFLOW_MASTER_IP_ADDRESS}
        cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            namespace: metallb-system
            name: config
          data:
            config: |
              address-pools:
              - name: default
                protocol: layer2
                addresses:
                - ${KUBEFLOW_MASTER_IP_ADDRESS}-${KUBEFLOW_MASTER_IP_ADDRESS}
        EOF
        sleep 10
        kubectl -n metallb-system wait --for=condition=Ready pods --all --timeout=60s

    - name: Install PlatIAgro for tests
      run: |
        curl -sL https://github.com/kubeflow/kfctl/releases/download/v1.2.0/kfctl_v1.2.0-0-gbc038f9_linux.tar.gz | tar xvz
        chmod +x kfctl
        export KF_NAME=platiagro
        export BASE_DIR=$(pwd)
        export KF_DIR=${BASE_DIR}/${KF_NAME}
        export CONFIG_URI="https://raw.githubusercontent.com/platiagro/manifests/v0.2.0-kubeflow-v1.2-branch/kfdef/kfctl_k8s_tests.v0.2.0.yaml"
        mkdir -p ${KF_DIR}
        cd ${KF_DIR}
        ../kfctl apply -V -f ${CONFIG_URI}

        # Removes resource requests because the VM has fewer CPUs than requests
        for NAMESPACE in $(kubectl get ns -o jsonpath='{.items[*].metadata.name}'); do
          kubectl -n ${NAMESPACE} get deployments -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}'| while read DEPLOYMENT; do
            kubectl -n ${NAMESPACE} patch deployment ${DEPLOYMENT} --type='json' --patch \
              '[{"op": "remove", "path": "/spec/template/spec/containers/0/resources"}]'
          done

          kubectl -n ${NAMESPACE} get statefulset -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}'| while read DEPLOYMENT; do
            kubectl -n ${NAMESPACE} patch statefulset ${DEPLOYMENT} --type='json' --patch \
              '[{"op": "remove", "path": "/spec/template/spec/containers/0/resources"}]'
          done
        done

        # Exposes MinIO for tests
        kubectl -n kubeflow patch svc minio-service --patch \
          '{ "spec": { "type": "NodePort", "ports": [ { "nodePort": 30000, "port": 9000, "protocol": "TCP", "targetPort": 9000 } ] } }'

        # Change Ingress from HTTPS to HTTP
        kubectl -n kubeflow patch gateway kubeflow-gateway --type='json' --patch \
          '[{"op": "remove", "path": "/spec/servers/0/tls"}, {"op": "replace", "path": "/spec/servers/0/port/name", "value": "http"}, {"op": "replace", "path": "/spec/servers/0/port/number", "value": 80}, {"op": "replace", "path": "/spec/servers/0/port/protocol", "value": "HTTP"}]'

        sleep 900
        kubectl -n kubeflow describe pod
        kubectl describe nodes
        #kubectl -n kubeflow wait --for=condition=Ready pods --all --timeout=120s
        #kubectl -n anonymous wait --for=condition=Ready pods --all --timeout=120s

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install .

    - name: Test with pytest
      run: |
        pip install pytest pytest-cov
        platiagro-init-db
        coverage erase
        coverage run --branch --source=projects -m pytest
        coverage xml -i
      env:
        MINIO_ENDPOINT: ${{ steps.lb.outputs.KUBEFLOW_MASTER_IP_ADDRESS }}:30000
        MINIO_ACCESS_KEY: minio
        MINIO_SECRET_KEY: minio123
        MYSQL_DB_HOST: localhost
        MYSQL_DB_NAME: platiagro
        MYSQL_DB_USER: root
        JUPYTER_ENDPOINT: http://${{ steps.lb.outputs.KUBEFLOW_MASTER_IP_ADDRESS }}/notebook/anonymous/server
        KF_PIPELINES_ENDPOINT: ${{ steps.lb.outputs.KUBEFLOW_MASTER_IP_ADDRESS }}/pipeline
        KF_PIPELINES_NAMESPACE: anonymous

    - name: SonarCloud Scan
      if: ${{ always() }}
      uses: sonarsource/sonarcloud-github-action@master
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}

    - name: Set output variables
      if: ${{ always() }}
      id: vars
      run: |
        VERSION=$(python -c "import projects; print(projects.__version__)")
        if [ ${{ github.ref }} = "refs/heads/master" ]; then
          echo ::set-output name=TAG::${VERSION}-SNAPSHOT
        elif [[ ${{ github.ref }} =~ ^refs/heads/v.*-branch$ ]]; then
          echo ::set-output name=TAG::${VERSION}
        else
          echo ::set-output name=TAG::${VERSION}-${{ github.sha }}
        fi
        echo ::set-output name=BRANCH::${{ github.ref }}
        echo ::set-output name=COMMIT::${{ github.sha }}

    - name: Build and push image
      uses: docker/build-push-action@v1
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}
        registry: registry.hub.docker.com
        repository: platiagro/projects
        tags: ${{ steps.vars.outputs.TAG }}
        build_args: COMMIT=${{ steps.vars.outputs.COMMIT }},BRANCH=${{ steps.vars.outputs.BRANCH }}
