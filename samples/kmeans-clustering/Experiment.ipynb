{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans Clustering - Experiment\n",
    "\n",
    "This is a component that trains a KMeans model using [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). \n",
    "<br>\n",
    "Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities.\n",
    "\n",
    "This notebook shows:\n",
    "- how to use SDK to load the dataset and save a model.\n",
    "- how to receive parameters from the platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare parameters\n",
    "Components may declare (and use) these default parameters:\n",
    "- dataset\n",
    "\n",
    "Use these parameters to load/save datasets, models, metrics, and figures with the help of [PlatIAgro SDK](https://platiagro.github.io/sdk/).\n",
    "\n",
    "You may also declare custom parameters to set when running an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dataset = \"iris\" #@param {type:\"string\"}\n",
    "n_clusters = 3 #@param {type:\"integer\"}\n",
    "n_init = 10 #@param {type:\"integer\"}\n",
    "max_iter = 300 #@param {type:\"integer\"}\n",
    "algorithm = \"auto\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Import and put the whole dataset in a pandas.DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro import load_dataset\n",
    "\n",
    "df = load_dataset(name=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata about the dataset\n",
    "For example, below we get the feature type for each column in the dataset. (eg. categorical, numerical, or datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from platiagro import stat_dataset\n",
    "\n",
    "metadata = stat_dataset(name=dataset)\n",
    "featuretypes = metadata[\"featuretypes\"]\n",
    "featuretypes = np.array(featuretypes)\n",
    "columns = df.columns.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro.featuretypes import NUMERICAL\n",
    "\n",
    "# Selects the indexes of numerical and non-numerical features\n",
    "numerical_indexes = np.where(featuretypes == NUMERICAL)[0]\n",
    "non_numerical_indexes = np.where(~(featuretypes == NUMERICAL))[0]\n",
    "\n",
    "# After the step handle_missing_values, \n",
    "# numerical features are grouped in the beggining of the array\n",
    "numerical_indexes_after_handle_missing_values = \\\n",
    "    np.arange(len(numerical_indexes))\n",
    "non_numerical_indexes_after_handle_missing_values = \\\n",
    "    np.arange(len(numerical_indexes), len(featuretypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a model using sklearn.cluster.KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "            (\"handle_missing_values\",\n",
    "             ColumnTransformer(\n",
    "                [(\"imputer_mean\", SimpleImputer(strategy=\"mean\"), numerical_indexes),\n",
    "                 (\"imputer_mode\", SimpleImputer(strategy=\"most_frequent\"), non_numerical_indexes)],\n",
    "                 remainder=\"drop\")),\n",
    "            (\"handle_categorical_features\",\n",
    "             ColumnTransformer(\n",
    "                 [(\"feature_encoder\", OrdinalEncoder(), non_numerical_indexes_after_handle_missing_values)],\n",
    "                 remainder=\"passthrough\")),\n",
    "            (\"estimator\", KMeans(n_clusters=n_clusters,\n",
    "                                    n_init=n_init,\n",
    "                                    max_iter=max_iter,\n",
    "                                    algorithm=algorithm))\n",
    "])\n",
    "\n",
    "X = pipeline.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the performance\n",
    "\n",
    "In the case of Kmeans we may measure performance by the silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "labels = pipeline.named_steps.estimator.labels_\n",
    "score = metrics.silhouette_score(X, labels, metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_std = StandardScaler().fit_transform(X.copy())",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(X_std)\n",
    "\n",
    "X_pca = pd.DataFrame(reduced, columns=[\"X\", \"Y\"])\n",
    "\n",
    "X_pca[\"Cluster\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ax = sns.scatterplot(x=\"X\", y=\"Y\", hue=\"Cluster\", data=X_pca)\n",
    "\n",
    "ax.set_title(\"PCA Graph\", {\"fontweight\": \"bold\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro import save_figure\n",
    "\n",
    "save_figure(figure=plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save metrics\n",
    "\n",
    "Record the metrics used to evaluate the model.<br>\n",
    "It's a good way to document the experiments, and also help to avoid running the same experiment twice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro import save_metrics\n",
    "\n",
    "save_metrics(silhouette_score=score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model\n",
    "\n",
    "Stores the model artifacts in a object storage.<br>\n",
    "It will make the model available for future deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro import save_model\n",
    "\n",
    "save_model(pipeline=pipeline, columns=columns)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "experiment_id": "da9c61f2-fd3f-4ba1-8f72-f47e81e83d6c",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "operator_id": "cd73629b-a548-4f10-8abc-22fe328aa3c9"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
