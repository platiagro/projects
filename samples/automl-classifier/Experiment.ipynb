{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML Classifier - Experiment\n",
    "\n",
    "This is a component that trains an AutoML Classifier model using [auto-sklearn](https://github.com/automl/auto-sklearn). \n",
    "<br>\n",
    "auto-sklearn is an automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator.\n",
    "\n",
    "This notebook shows:\n",
    "- how to use the [SDK](https://platiagro.github.io/sdk/) to load datasets, save models and other artifacts.\n",
    "- how to declare parameters and use them to build reusable components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare parameters and model hyperparameters\n",
    "Components may declare (and use) these default parameters:\n",
    "- dataset\n",
    "- target\n",
    "\n",
    "Use these parameters to load/save datasets, models, metrics, and figures with the help of [PlatIAgro SDK](https://platiagro.github.io/sdk/). <br />\n",
    "You may also declare custom parameters to set when running an experiment.\n",
    "\n",
    "Select the hyperparameters and their respective values to be used when training the model:\n",
    "- time_left_for_this_task\n",
    "- per_run_time_limit\n",
    "- ensemble_size\n",
    "\n",
    "These parameters are just a few offered by the model class, you may also use another existing parameter. <br />\n",
    "Check the [model parameters](https://automl.github.io/auto-sklearn/master/api.html#autosklearn.classification.AutoSklearnClassifier) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "dataset = \"iris\" #@param {type:\"string\"}\n",
    "target = \"Species\" #@param {type:\"feature\", label:\"Atributo alvo\", description: \"Seu modelo será treinado para prever os valores do alvo.\"}\n",
    "\n",
    "# features to apply One Hot Encoder\n",
    "one_hot_features = \"\" #@param {type:\"feature\",multiple:true,label:\"Features para fazer codificação one-hot\", description: \"Seu modelo utilizará a codificação one-hot para as features selecionadas. As demais features categóricas serão codificadas utilizando a codificação ordinal.\"}\n",
    "\n",
    "# hyperparameters\n",
    "time_left_for_this_task = 60 #@param {type:\"integer\", label:\"Limite de tempo (em segundos)\", description:\"Limite de tempo para a procura de modelos apropriados\"}\n",
    "per_run_time_limit = 60 #@param {type:\"integer\", label:\"Limite de tempo (em segundos)\", description:\"Prazo para uma única chamada para o modelo de Machine Learning\"}\n",
    "ensemble_size = 50 #@param {type:\"integer\", label:\"Ensemble Learning\", description:\"Número de modelos adicionados ao conjunto criado pela seleção do Ensemble das bibliotecas de modelos\"}\n",
    "\n",
    "# predict method\n",
    "method = \"predict_proba\" #@param [\"predict_proba\", \"predict\"] {type:\"string\", label:\"Método de Predição\", description:\"Se optar por 'predict_proba', o método de predição será a probabilidade estimada de cada classe, já o 'predict' prediz a qual classe pertence\"} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Import and put the whole dataset in a pandas.DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro import load_dataset\n",
    "\n",
    "df = load_dataset(name=dataset)\n",
    "X = df.drop(target, axis=1).to_numpy()\n",
    "y = df[target].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata about the dataset\n",
    "For example, below we get the feature type for each column in the dataset. (eg. categorical, numerical, or datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from platiagro import stat_dataset\n",
    "\n",
    "metadata = stat_dataset(name=dataset)\n",
    "featuretypes = metadata[\"featuretypes\"]\n",
    "\n",
    "columns = df.columns.to_numpy()\n",
    "featuretypes = np.array(featuretypes)\n",
    "target_index = np.argwhere(columns == target)\n",
    "columns = np.delete(columns, target_index)\n",
    "featuretypes = np.delete(featuretypes, target_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode target labels\n",
    "\n",
    "The target labels are converted to ordinal integers with value between 0 and n_classes-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train/test splits\n",
    "\n",
    "Training Dataset: the sample of data used to fit the model.\n",
    "\n",
    "Test Dataset: the sample of data used to provide an unbiased evaluation of a model fit on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro.featuretypes import NUMERICAL\n",
    "\n",
    "# Selects the indexes of numerical and non-numerical features\n",
    "numerical_indexes = np.where(featuretypes == NUMERICAL)[0]\n",
    "non_numerical_indexes = np.where(~(featuretypes == NUMERICAL))[0]\n",
    "\n",
    "\n",
    "# Selects non-numerical features to apply ordinal encoder or one-hot encoder\n",
    "one_hot_features = np.asarray(one_hot_features)\n",
    "non_numerical_indexes_one_hot = np.where(~(featuretypes == NUMERICAL) & np.isin(columns,one_hot_features))[0]\n",
    "non_numerical_indexes_ordinal = np.where(~(featuretypes == NUMERICAL) & ~(np.isin(columns,one_hot_features)))[0]\n",
    "\n",
    "\n",
    "# After the step handle_missing_values, \n",
    "# numerical features are grouped in the beggining of the array\n",
    "numerical_indexes_after_handle_missing_values = \\\n",
    "    np.arange(len(numerical_indexes))\n",
    "non_numerical_indexes_after_handle_missing_values = \\\n",
    "    np.arange(len(numerical_indexes), len(featuretypes))\n",
    "one_hot_indexes_after_handle_missing_values = non_numerical_indexes_after_handle_missing_values[np.where(np.isin(non_numerical_indexes,non_numerical_indexes_one_hot))[0]]         \n",
    "ordinal_indexes_after_handle_missing_values = non_numerical_indexes_after_handle_missing_values[np.where(np.isin(non_numerical_indexes,non_numerical_indexes_ordinal))[0]]                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a model using autosklearn.classification.AutoSklearnClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from autosklearn.classification import AutoSklearnClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('handle missing values',\n",
    "     ColumnTransformer(\n",
    "        [('imputer_mean', SimpleImputer(strategy='mean'), numerical_indexes),\n",
    "         ('imputer_mode', SimpleImputer(strategy='most_frequent'), non_numerical_indexes)],\n",
    "         remainder='drop')),\n",
    "    ('handle categorical features',\n",
    "    ColumnTransformer(\n",
    "         [('feature_encoder_ordinal', OrdinalEncoder(), ordinal_indexes_after_handle_missing_values),\n",
    "          ('feature_encoder_onehot', OneHotEncoder(), one_hot_indexes_after_handle_missing_values)],\n",
    "         remainder='passthrough')),\n",
    "    ('estimator', AutoSklearnClassifier(time_left_for_this_task=time_left_for_this_task,\n",
    "                                        per_run_time_limit=per_run_time_limit,\n",
    "                                        ensemble_size=ensemble_size)),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "X_train = Pipeline(pipeline.steps[:-1]).transform(X_train)\n",
    "pipeline.named_steps.estimator.refit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the performance\n",
    "The [**Confusion Matrix**](https://en.wikipedia.org/wiki/Confusion_matrix) is a performance measurement for machine learning classification.<br>\n",
    "It is extremely useful for measuring [Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification), [Recall, Precision, and F-measure](https://en.wikipedia.org/wiki/Precision_and_recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "# uses the model to make predictions on the Test Dataset\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_prob = pipeline.predict_proba(X_test)\n",
    "\n",
    "# computes confusion matrix\n",
    "labels = np.unique(y)\n",
    "data = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "# computes precision, recall, f1-score, support (for multiclass classification problem) and accuracy\n",
    "if len(labels)>2:  #multiclass classification\n",
    "    p, r, f1, s = precision_recall_fscore_support(y_test, y_pred,\n",
    "                                                  labels=labels,\n",
    "                                                  average=None)\n",
    "    \n",
    "    commom_metrics = pd.DataFrame(data=zip(p, r, f1, s),columns=['Precision','Recall','F1-Score','Support']) \n",
    "    \n",
    "    average_options = ('micro', 'macro', 'weighted')\n",
    "    for average in average_options:\n",
    "        if average.startswith('micro'):\n",
    "            line_heading = 'accuracy'\n",
    "        else:\n",
    "            line_heading = average + ' avg'\n",
    "\n",
    "        # compute averages with specified averaging method\n",
    "        avg_p, avg_r, avg_f1, _ = precision_recall_fscore_support(\n",
    "            y_test, y_pred, labels=labels,\n",
    "            average=average)\n",
    "        avg = pd.Series({'Precision':avg_p,  'Recall':avg_r,  'F1-Score':avg_f1,  'Support':np.sum(s)},name=line_heading)\n",
    "        commom_metrics = commom_metrics.append(avg)\n",
    "else: #binary classification\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred,\n",
    "                                                  average='binary')\n",
    "    accuracy=accuracy_score(y_test, y_pred)\n",
    "    commom_metrics = pd.DataFrame(data={'Precision':p,'Recall':r,'F1-Score':f1,'Accuracy':accuracy},index=[1])\n",
    "\n",
    "# puts matrix in pandas.DataFrame for better format\n",
    "labels = label_encoder.inverse_transform(labels)\n",
    "confusion_matrix = pd.DataFrame(data, columns=labels, index=labels)\n",
    "\n",
    "# add correct index labels to commom_metrics DataFrame (for multiclass classification)\n",
    "if len(labels)>2:\n",
    "    as_list = commom_metrics.index.tolist()\n",
    "    as_list[0:len(labels)] = labels\n",
    "    commom_metrics.index = as_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save metrics\n",
    "\n",
    "Record the metrics used to evaluate the model.<br>\n",
    "It's a good way to document the experiments, and also help to avoid running the same experiment twice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro import save_metrics\n",
    "\n",
    "save_metrics(confusion_matrix=confusion_matrix,commom_metrics=commom_metrics)\n",
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save figure\n",
    "\n",
    "Compute and plot Compute Receiver operating characteristic (ROC) curve to evaluate the model performance. It illustrates the performance of a binary classifier system as its discrimination threshold is varied. For multiclass classification task, it is used the one-vs-rest algorithm, that is, computes the AUC of each class against the rest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import preprocessing\n",
    "from numpy import unique\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "y_prob = pipeline.predict_proba(X_test)\n",
    "\n",
    "def plot_roc_curve(y_test,y_prob,labels):\n",
    "    n_classes = len(labels)\n",
    "    \n",
    "    if n_classes == 2:\n",
    "        # Compute ROC curve \n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)  \n",
    "        \n",
    "        # Plot ROC Curve\n",
    "        plt.figure()\n",
    "        lw = 2\n",
    "        plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([-0.01, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "    else:  \n",
    "        # Binarize the output\n",
    "        lb = preprocessing.LabelBinarizer()\n",
    "        y_test_bin = lb.fit_transform(y_test)\n",
    "\n",
    "        # Compute ROC curve for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()  \n",
    "\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_prob[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        color=cm.rainbow(np.linspace(0,1,n_classes+1))\n",
    "        plt.figure()\n",
    "        lw = 2\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([-0.01, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        \n",
    "        for i,c in zip(range(n_classes),color):                   \n",
    "            plt.plot(fpr[i], tpr[i], color=c,\n",
    "             lw=lw, label='ROC curve - Class %s (area = %0.2f)' % (labels[i] ,roc_auc[i]))\n",
    "            plt.title('ROC Curve One-vs-Rest')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "from platiagro import save_figure\n",
    "from platiagro import list_figures\n",
    "\n",
    "plot_roc_curve(y_test,y_prob,labels)\n",
    "\n",
    "save_figure(figure=plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model\n",
    "\n",
    "Stores the model artifacts in a object storage.<br>\n",
    "It will make the model available for future deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro import save_model\n",
    "\n",
    "save_model(pipeline=pipeline,\n",
    "           label_encoder=label_encoder,\n",
    "           columns=columns,\n",
    "           method=method)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "experiment_id": "48f2668a-e31a-4b5a-a91a-28fd649f7adc",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "operator_id": "64401802-2785-4f26-85c8-d7974fc78454"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
