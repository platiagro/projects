{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Selection - Experiment\n",
    "\n",
    "Remove specifics features from dataset.\n",
    "\n",
    "This notebook shows:\n",
    "- how to use the [SDK](https://platiagro.github.io/sdk/) to load datasets, save models and other artifacts.\n",
    "- how to declare parameters and use them to build reusable components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare parameters and model hyperparameters\n",
    "Components may declare (and use) these default parameters:\n",
    "- dataset\n",
    "\n",
    "Use these parameters to load/save datasets, models, metrics, and figures with the help of [PlatIAgro SDK](https://platiagro.github.io/sdk/). <br />\n",
    "You may also declare custom parameters to set when running an experiment.\n",
    "\n",
    "Select the hyperparameters and their respective values to be used when training the model:\n",
    "- features_to_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "dataset = \"iris\" #@param {type:\"string\"}\n",
    "target = \"Species\" #@param {type:\"feature\", label:\"Atributo alvo\", description: \"Seu modelo serÃ¡ treinado para prever os valores do alvo.\"}\n",
    "\n",
    "# hyperparameters\n",
    "features_to_filter = [\"SepalLengthCm\"] #@param {type:\"feature\", multiple:true, label:\"Features Para Filtragem\", description:\"Remove features selecionadas do dataset.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Import and put the whole dataset in a pandas.DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro import load_dataset\n",
    "\n",
    "df = load_dataset(name=dataset)\n",
    "X = df.drop(target, axis=1).to_numpy()\n",
    "y = df[target].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata about the dataset\n",
    "For example, below we get the feature type for each column in the dataset. (eg. categorical, numerical, or datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from platiagro import stat_dataset\n",
    "\n",
    "metadata = stat_dataset(name=dataset)\n",
    "featuretypes = metadata[\"featuretypes\"]\n",
    "\n",
    "columns = df.columns.to_numpy()\n",
    "featuretypes = np.array(featuretypes)\n",
    "target_index = np.argwhere(columns == target)\n",
    "columns = np.delete(columns, target_index)\n",
    "featuretypes = np.delete(featuretypes, target_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping custom transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile CustomTransformer.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class Filter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Feature selector that removes specific features.\n",
    "    \n",
    "    This feature selection algorithm looks only at the columns\n",
    "    and then remove the selected ones.\n",
    "    \n",
    "    Attributes:\n",
    "        features_to_remove: A list of features to be removed.\n",
    "        dataset_columns: An np.ndarray with the current features of the dataset.\n",
    "        drop_indexes: An list of indexes to be droped.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features_to_remove: list, dataset_columns: np.ndarray):\n",
    "        \"\"\"Inits Filter class.\n",
    "        \n",
    "        Args:\n",
    "            features: features to be removed.\n",
    "            columns: columns of the dataset.\n",
    "        \"\"\"\n",
    "        self.features_to_remove = features_to_remove\n",
    "        self.dataset_columns = dataset_columns\n",
    "    \n",
    "    def transform(self, X: np.ndarray):\n",
    "        \"\"\"Reduce X to the selected features.\n",
    "        \n",
    "        Args:\n",
    "            X: the input samples.\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: the input samples with only the selected features.\n",
    "        \"\"\"\n",
    "        return np.delete(X, self.drop_indexes, axis=1)\n",
    "    \n",
    "    def fit(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Fit the model.\n",
    "        \n",
    "        Learn selected features index.\n",
    "        \n",
    "        Args:\n",
    "            X: the imput sample.\n",
    "        \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        if set(self.features_to_remove).issubset(self.dataset_columns):\n",
    "            self.drop_indexes = np.where(np.in1d(self.dataset_columns,\n",
    "                                                 self.features_to_remove))[0]\n",
    "            return self\n",
    "        else:\n",
    "            raise ValueError(f\"Dataset has no {self.features_to_remove} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from CustomTransformer import Filter\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Call changes\n",
    "pipeline = make_pipeline(\n",
    "    Filter(features_to_remove=features_to_filter,\n",
    "           dataset_columns=columns)\n",
    ")\n",
    "\n",
    "# Train model and transform dataset\n",
    "X = pipeline.fit_transform(X)\n",
    "\n",
    "# Put back in pd.DataFrame\n",
    "features_after_pipeline = columns[~np.in1d(columns, features_to_filter)]\n",
    "df = pd.DataFrame(X, columns=features_after_pipeline)\n",
    "df[target] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset\n",
    "\n",
    "Stores the transformed dataset in a object storage.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro import save_dataset\n",
    "\n",
    "save_dataset(name=dataset, df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model\n",
    "\n",
    "Stores the model artifacts in a object storage.<br>\n",
    "It will make the model available for future deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platiagro import save_model\n",
    "\n",
    "save_model(pipeline=pipeline,\n",
    "           columns=columns,\n",
    "           features_after_pipeline=features_after_pipeline)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "experiment_id": "baa4dfa2-9ebb-11ea-bb37-0242ac130002",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "operator_id": "c2dc16f4-9ebb-11ea-bb37-0242ac130002"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
